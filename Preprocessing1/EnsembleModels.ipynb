{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65cfacd3",
   "metadata": {},
   "source": [
    "### Ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27b98a",
   "metadata": {},
   "source": [
    "Installing python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas scikit-learn dask \"dask[dataframe]\" seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb6c4a",
   "metadata": {},
   "source": [
    "Importing python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855325a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e57bde",
   "metadata": {},
   "source": [
    "Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ada42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '../Datasets/Small_datasetPreprocessed1.parquet'\n",
    "\n",
    "if os.path.exists(dataset):\n",
    "    df = pd.read_parquet(dataset)\n",
    "    df = df.dropna()\n",
    "else:\n",
    "    print(\"Dataset not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950b4ede",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244830c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train, test, validation sets\n",
    "X = df.drop(['fare_amount'], axis=1)\n",
    "y = df['fare_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af019df",
   "metadata": {},
   "source": [
    "Functions to plot the models results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55007e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid_search_heatmap(grid_search, param1, param2, metric=\"mean_test_score\"):\n",
    "    results = grid_search.cv_results_\n",
    "    scores = results[metric].reshape(len(grid_search.param_grid[param1]), len(grid_search.param_grid[param2]))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(scores, annot=True, fmt=\".3f\", xticklabels=grid_search.param_grid[param2],\n",
    "                yticklabels=grid_search.param_grid[param1], cmap=\"viridis\")\n",
    "    plt.xlabel(param2)\n",
    "    plt.ylabel(param1)\n",
    "    plt.title(f\"GridSearchCV Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions_vs_true(y_true, y_pred, model_name=\"Model\"):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, color=\"blue\", label=\"Predictions\")\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color=\"red\", linestyle=\"--\", label=\"Ideal Fit\")\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"{model_name}: Predictions vs True Values\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def plot_model_comparison(models, metrics, metric_name=\"R2 Score\"):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(models, metrics, color=[\"blue\", \"orange\", \"green\", \"purple\"])\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f\"Comparison of {metric_name} Across Models\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    y_min = math.floor(min(metrics) / 0.05) * 0.05\n",
    "    y_max = max(max(metrics), 1.0)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    for i, v in enumerate(metrics):\n",
    "        plt.text(i, v + 0.002, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dee9a",
   "metadata": {},
   "source": [
    "Random Forest Regressor ensemble model with grid search for the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f401960",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 30, 50],\n",
    "    'max_features': [None, 5, 10],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R^2 Score:\", grid_search.best_score_)\n",
    "\n",
    "plot_grid_search_heatmap(grid_search, \"n_estimators\", \"max_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90ce817",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_rf = RandomForestRegressor(n_estimators=50, max_features=5, random_state=42)\n",
    "bagging_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bagging_rf.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_random_forest = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R^2 Score:\", r2_random_forest)\n",
    "\n",
    "plot_predictions_vs_true(y_test, y_pred, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2ba2d2",
   "metadata": {},
   "source": [
    "AdaBoost Regressor ensemble model with grid search for the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 30, 50],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=AdaBoostRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R^2 Score:\", grid_search.best_score_)\n",
    "\n",
    "plot_grid_search_heatmap(grid_search, \"n_estimators\", \"learning_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ded4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_ada = AdaBoostRegressor(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "boosting_ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred = boosting_ada.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_adaboost = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R^2 Score:\", r2_adaboost)\n",
    "\n",
    "plot_predictions_vs_true(y_test, y_pred, \"AdaBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8fa16c",
   "metadata": {},
   "source": [
    "Gradient Boosting Regressor ensemble model with grid search for the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5798bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 30, 50],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "}\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best R^2 Score:\", grid_search.best_score_)\n",
    "\n",
    "plot_grid_search_heatmap(grid_search, \"n_estimators\", \"learning_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16145fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_grad = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "boosting_grad.fit(X_train, y_train)\n",
    "\n",
    "y_pred = boosting_grad.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_gradboost = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R^2 Score:\", r2_gradboost)\n",
    "\n",
    "plot_predictions_vs_true(y_test, y_pred, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351cc808",
   "metadata": {},
   "source": [
    "Stacking ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ecb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base and meta models\n",
    "base_learners = [\n",
    "    ('dt', DecisionTreeRegressor(random_state=42)),\n",
    "    ('el', ElasticNet(alpha=0.1, l1_ratio=0.5))\n",
    "]\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_model\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2_stacking = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R^2 Score:\", r2_stacking)\n",
    "\n",
    "plot_predictions_vs_true(y_test, y_pred, \"Stacking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d0282",
   "metadata": {},
   "source": [
    "Models evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a3954",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Random Forest\", \"AdaBoost\", \"Gradient Boosting\", \"Stacking\"]\n",
    "metrics = [r2_random_forest, r2_adaboost, r2_gradboost, r2_stacking]\n",
    "plot_model_comparison(models, metrics, \"R2 Score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
