{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is a kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas scikit-learn dask \"dask[dataframe]\" seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data in the 2 created datasets (2019newBig.csv: 12M rows, 2019new.csv: 1.2M rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the DB and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '../Datasets/2019newPreprocessed1.parquet'\n",
    "\n",
    "if os.path.exists(dataset):\n",
    "    df = pd.read_parquet(dataset)\n",
    "    df = df.dropna() \n",
    "    print(df.head(1))\n",
    "    print(df.shape)\n",
    "else:\n",
    "    print(\"Dataset not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start models training with different NN and parameter to see the best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define the neural network model\n",
    "def NN(input_size):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, 128)\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n",
    "\n",
    "# Custom dataset class\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Preprocess the data\n",
    "input_size = df.shape[1] - 1\n",
    "X = df.drop(['fare_amount'], axis=1).values\n",
    "y = df['fare_amount'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = NYCTaxiDataset(X_train, y_train)\n",
    "test_dataset = NYCTaxiDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 8196\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=24, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=24, pin_memory=True)\n",
    "\n",
    "# Initialize the model and wrap it with DataParallel\n",
    "model = NN(input_size).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with progress bar\n",
    "def train_model(model, train_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch [{epoch+1}/{num_epochs}]', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function for regression\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test RMSE: {mse ** 0.5:.2f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model_Bignew.pth')\n",
    "\n",
    "print(\"Training and evaluation completed!\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Larger layer with 512 neurons\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)         # Another large layer with 256 neurons            \n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)         # 128 neurons\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 64)          # 64 neurons\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc5 = nn.Linear(64, 32)           # 32 neurons\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.fc6 = nn.Linear(32, 1)            # Output layer\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)         # Increased dropout rate to combat overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Preprocess the data\n",
    "input_size = df.shape[1] - 1\n",
    "X = df.drop(['fare_amount'], axis=1).values\n",
    "y = df['fare_amount'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = NYCTaxiDataset(X_train, y_train)\n",
    "test_dataset = NYCTaxiDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "# Initialize the model and wrap it with DataParallel if multiple GPUs are available\n",
    "model = Net(input_size)\n",
    "model = model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop with TF32 enabled\n",
    "def train_model(model, train_loader, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch [{epoch+1}/{num_epochs}]', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        scheduler.step(epoch_loss)\n",
    "        if(epoch%10==0):\n",
    "            name = f'Models/model_LargeNN_{epoch // 10}.pth'\n",
    "            torch.save(model, name)\n",
    "            evaluate_model(model, test_loader)\n",
    "\n",
    "# Evaluation function for regression\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test RMSE: {mse ** 0.5:.2f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model, 'model_LargeNN.pth')\n",
    "\n",
    "print(\"Training and evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Larger layer with 512 neurons\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)         # Another large layer with 256 neurons            \n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)         # 128 neurons\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 64)          # 64 neurons\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc5 = nn.Linear(64, 32)           # 32 neurons\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.fc6 = nn.Linear(32, 1)            # Output layer\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)         # Increased dropout rate to combat overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn5(self.fc5(x)))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "# Custom dataset class\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Preprocess the data\n",
    "input_size = df.shape[1] - 1\n",
    "X = df.drop(['fare_amount'], axis=1).values\n",
    "y = df['fare_amount'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = NYCTaxiDataset(X_train, y_train)\n",
    "test_dataset = NYCTaxiDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 512\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "# Function to load the entire model\n",
    "def load_entire_model(filepath):\n",
    "    model = torch.load(filepath, map_location=device)  # Load the entire model\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Evaluation function for regression with R² score, MAE, and MSE\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mae = mean_absolute_error(actuals, predictions)  # Calculate MAE\n",
    "    r2 = r2_score(actuals, predictions)  # Calculate R² score\n",
    "    \n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test RMSE: {mse ** 0.5:.2f}\")\n",
    "    print(f\"Test MAE: {mae:.2f}\")\n",
    "    print(f\"Test R² Score: {r2:.4f}\")\n",
    "\n",
    "# Load the entire model and evaluate it\n",
    "model_filepath = 'model_LargeNN.pth'  # Replace with your .pth file path\n",
    "loaded_model = load_entire_model(model_filepath)\n",
    "\n",
    "# Assume test_loader is defined as before\n",
    "evaluate_model(loaded_model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1062312,
     "sourceId": 1793773,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30042,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
