{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is a kernel with starter code demonstrating how to read in the data and begin exploring. Click the blue \"Edit Notebook\" or \"Fork Notebook\" button at the top of this kernel to begin editing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "\n",
    "To begin this exploratory analysis, first use `matplotlib` to import libraries and define functions for plotting the data. Depending on the data, not all plots will be made. (Hey, I'm just a kerneling bot, not a Kaggle Competitions Grandmaster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib numpy pandas scikit-learn dask \"dask[dataframe]\" seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt # plotting\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data in the 2 created datasets (2019newBig.csv: 12M rows, 2019new.csv: 1.2M rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the DB and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset = 'Datasets/2019new.csv'\n",
    "\n",
    "if os.path.exists(dataset):\n",
    "    # Load the CSV using dask for parallel processing\n",
    "    df = dd.read_csv(dataset).compute()\n",
    "    \n",
    "    # If 'congestion_surcharge' column has NaNs, set them to 0\n",
    "    df['congestion_surcharge'] = df['congestion_surcharge'].fillna(0)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Drop rows where 'trip_distance' is 0 and 'fare_amount' is <= 0\n",
    "    df = df.drop(df[(df['trip_distance'] == 0)].index)\n",
    "    df = df.drop(['total_amount'], axis=1)\n",
    "    df = df.drop(['extra'], axis=1)\n",
    "    df = df.drop(['mta_tax'], axis=1)\n",
    "    df = df.drop(['tip_amount'], axis=1)\n",
    "    df = df.drop(['tolls_amount'], axis=1)\n",
    "    df = df.drop(['improvement_surcharge'], axis=1)\n",
    "    df = df.drop(['congestion_surcharge'], axis=1)\n",
    "    df = df.drop(['store_and_fwd_flag'], axis=1)\n",
    "    df = df.drop(['payment_type'], axis=1)\n",
    "    df = df[df['fare_amount'] > 0]\n",
    "    df = df[df['fare_amount'] <= 50]\n",
    "\n",
    "    # Convert 'tpep_pickup_datetime' to datetime and extract date and hour\n",
    "    df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['pickup_date'] = df['tpep_pickup_datetime'].dt.date\n",
    "    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "    # Drop unneeded columns\n",
    "    df = df.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1)\n",
    "\n",
    "    # Plotting distribution of rides over date\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['pickup_date'].value_counts().sort_index().plot().set_xlim(pd.Timestamp('2019-01-01'), pd.Timestamp('2019-12-31'))\n",
    "    plt.title('Distribution of rides over date')\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting distribution of rides over hour of the day\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['pickup_hour'].value_counts().sort_index().plot()\n",
    "    plt.title('Distribution of rides over hour of the day')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Dataset not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup weather and holiday database and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather = pd.read_csv('Datasets/weather.csv')\n",
    "\n",
    "weather=weather.drop(columns=['tmax','tmin','departure','HDD','CDD'])\n",
    "weather['date'] = pd.to_datetime(weather['date'])\n",
    "\n",
    "holidays = pd.read_csv('Datasets/USHoliday.csv')\n",
    "\n",
    "#maintain only if holiday is in 2019\n",
    "holidays['Date'] = pd.to_datetime(holidays['Date'])\n",
    "holidays=holidays[holidays['Date'].dt.year==2019]\n",
    "\n",
    "#set precipitation to 0 if NaN and integer, new_snow, snow_depth\n",
    "weather['precipitation'] = weather['precipitation'].replace(to_replace=\"T\", value=0)\n",
    "weather['new_snow'] = weather['new_snow'].replace(to_replace=\"T\", value=0)\n",
    "weather['snow_depth'] = weather['snow_depth'].replace(to_replace=\"T\", value=0)\n",
    "\n",
    "#set to float\n",
    "weather['precipitation'] = weather['precipitation'].astype(float)\n",
    "weather['new_snow'] = weather['new_snow'].astype(float)\n",
    "weather['snow_depth'] = weather['snow_depth'].astype(float)\n",
    "\n",
    "# Ensure the pickup_date column is in datetime64[ns] format\n",
    "df['pickup_date'] = pd.to_datetime(df['pickup_date'])\n",
    "\n",
    "new_df = pd.merge(df, weather, how='left', left_on='pickup_date', right_on='date')\n",
    "\n",
    "new_df = new_df.drop(['date'], axis=1)\n",
    "#add column 1 if week day, 2 if weekend, 3 if holiday\n",
    "new_df['holiday'] = new_df['pickup_date'].isin(holidays['Date']).astype(int)\n",
    "new_df['day_of_week'] = new_df['pickup_date'].dt.dayofweek\n",
    "new_df['day_type'] = np.where(new_df['day_of_week'] < 5, 1, 2)\n",
    "new_df.loc[new_df['holiday'] == 1, 'day_type'] = 3\n",
    "new_df = new_df.drop(['pickup_date'], axis=1)\n",
    "new_df = new_df.drop(['day_of_week'], axis=1)\n",
    "new_df = new_df.drop(['holiday'], axis=1)\n",
    "\n",
    "#print first row full data not truncated\n",
    "\n",
    "new_df = new_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unifing with the zones database and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = pd.read_csv('Datasets/taxi_zone_lookup.csv')\n",
    "zones = zones.drop(['Borough'], axis=1)\n",
    "zones = zones.drop(['Zone'], axis=1)\n",
    "\n",
    "zones = zones[zones['service_zone'] != 'N/A']\n",
    "\n",
    "# Replace 'EWR' with 'Airports' in the 'service_zone' column\n",
    "zones['service_zone'] = zones['service_zone'].replace('EWR', 'Airports')\n",
    "\n",
    "# Merge taxi_zone_lookup.csv with the new dataset on 'pulocationid' and 'dolocationid'\n",
    "pulocation = new_df.merge(zones[['LocationID', 'service_zone']], left_on='pulocationid', right_on='LocationID', how='left')\n",
    "dolocation = pulocation.merge(zones[['LocationID', 'service_zone']], left_on='dolocationid', right_on='LocationID', how='left', suffixes=('_pulocation', '_dolocation'))\n",
    "\n",
    "# Create a new column 'zone_type' based on the conditions\n",
    "def get_zone_type(row):\n",
    "    service_zone_pulocation = row['service_zone_pulocation']\n",
    "    service_zone_dolocation = row['service_zone_dolocation']\n",
    "\n",
    "    if service_zone_pulocation == 'Airports' or service_zone_dolocation == 'Airports':\n",
    "        return 1\n",
    "    elif 'Boro Zone' in [service_zone_pulocation, service_zone_dolocation]:\n",
    "        return 2\n",
    "    elif 'Yellow Zone' in [service_zone_pulocation, service_zone_dolocation]:\n",
    "        return 3\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the zone_type function to the merged dataframe\n",
    "dolocation['zone_type'] = dolocation.apply(get_zone_type, axis=1)\n",
    "\n",
    "# Remove rows where 'zone_type' is None (rows that don't meet any of the conditions)\n",
    "new_df = dolocation.dropna(subset=['zone_type'])\n",
    "\n",
    "new_df = new_df.drop(['pulocationid'], axis=1)\n",
    "new_df = new_df.drop(['dolocationid'], axis=1)\n",
    "new_df = new_df.drop(['LocationID_pulocation'], axis=1)\n",
    "new_df = new_df.drop(['service_zone_pulocation'], axis=1)\n",
    "new_df = new_df.drop(['LocationID_dolocation'], axis=1)\n",
    "new_df = new_df.drop(['service_zone_dolocation'], axis=1)\n",
    "\n",
    "print(new_df.head(1))\n",
    "print(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(['tavg'], axis=1)\n",
    "new_df = new_df.drop(['precipitation'], axis=1)\n",
    "new_df = new_df.drop(['new_snow'], axis=1)\n",
    "new_df = new_df.drop(['snow_depth'], axis=1)\n",
    "\n",
    "print(new_df.head(1))\n",
    "print(new_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start models training with different NN and parameter to see the best ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define the neural network model\n",
    "def NN(input_size):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, 128)\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n",
    "\n",
    "# Custom dataset class\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Preprocess the data\n",
    "input_size = new_df.shape[1] - 1\n",
    "X = new_df.drop(['fare_amount'], axis=1).values\n",
    "y = new_df['fare_amount'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = NYCTaxiDataset(X_train, y_train)\n",
    "test_dataset = NYCTaxiDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=24, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=24, pin_memory=True)\n",
    "\n",
    "# Initialize the model and wrap it with DataParallel\n",
    "model = NN(input_size).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with progress bar\n",
    "def train_model(model, train_loader, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch [{epoch+1}/{num_epochs}]', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function for regression\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test RMSE: {mse ** 0.5:.2f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model_Bignew.pth')\n",
    "\n",
    "print(\"Training and evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the neural network model with batch normalization and dropout\n",
    "def NN(input_size):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, 128)\n",
    "            self.bn1 = nn.BatchNorm1d(128)\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.bn2 = nn.BatchNorm1d(64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.bn2(self.fc2(x)))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n",
    "\n",
    "# Custom dataset class\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Preprocess the data\n",
    "input_size = new_df.shape[1] - 1\n",
    "X = new_df.drop(['fare_amount'], axis=1).values\n",
    "y = new_df['fare_amount'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = NYCTaxiDataset(X_train, y_train)\n",
    "test_dataset = NYCTaxiDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=24, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=24, pin_memory=True)\n",
    "\n",
    "# Initialize the model and wrap it with DataParallel\n",
    "model = NN(input_size).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop with mixed precision\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch [{epoch+1}/{num_epochs}]', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "# Evaluation function for regression\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test RMSE: {mse ** 0.5:.2f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model_Bignew.pth')\n",
    "\n",
    "print(\"Training and evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Enable BF16 training if supported\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32\n",
    "\n",
    "# Define the neural network model with batch normalization and dropout\n",
    "def NN(input_size):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, 128)\n",
    "            self.bn1 = nn.BatchNorm1d(128)\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.bn2 = nn.BatchNorm1d(64)\n",
    "            self.fc3 = nn.Linear(64, 1)\n",
    "            self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.bn2(self.fc2(x)))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n",
    "\n",
    "# Custom dataset class\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Preprocess the data\n",
    "input_size = new_df.shape[1] - 1\n",
    "X = new_df.drop(['fare_amount'], axis=1).values\n",
    "y = new_df['fare_amount'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = NYCTaxiDataset(X_train, y_train)\n",
    "test_dataset = NYCTaxiDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=24, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=24, pin_memory=True)\n",
    "\n",
    "# Initialize the model and wrap it with DataParallel\n",
    "model = NN(input_size).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop with bf16 precision\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch [{epoch+1}/{num_epochs}]', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "# Evaluation function for regression\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test RMSE: {mse ** 0.5:.2f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model_BF16.pth')\n",
    "\n",
    "print(\"Training and evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a larger neural network model\n",
    "def LargeNN(input_size):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, 512)  # Larger layer with 512 neurons\n",
    "            self.bn1 = nn.BatchNorm1d(512)\n",
    "            self.fc2 = nn.Linear(512, 256)         # Another large layer with 256 neurons\n",
    "            self.bn2 = nn.BatchNorm1d(256)\n",
    "            self.fc3 = nn.Linear(256, 128)         # 128 neurons\n",
    "            self.bn3 = nn.BatchNorm1d(128)\n",
    "            self.fc4 = nn.Linear(128, 64)          # 64 neurons\n",
    "            self.bn4 = nn.BatchNorm1d(64)\n",
    "            self.fc5 = nn.Linear(64, 32)           # 32 neurons\n",
    "            self.bn5 = nn.BatchNorm1d(32)\n",
    "            self.fc6 = nn.Linear(32, 1)            # Output layer\n",
    "\n",
    "            self.dropout = nn.Dropout(0.4)         # Increased dropout rate to combat overfitting\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.bn2(self.fc2(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.bn3(self.fc3(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.bn4(self.fc4(x)))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.bn5(self.fc5(x)))\n",
    "            x = self.fc6(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n",
    "\n",
    "# Custom dataset class\n",
    "class NYCTaxiDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Preprocess the data\n",
    "input_size = new_df.shape[1] - 1\n",
    "X = new_df.drop(['fare_amount'], axis=1).values\n",
    "y = new_df['fare_amount'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
    "\n",
    "train_dataset = NYCTaxiDataset(X_train, y_train)\n",
    "test_dataset = NYCTaxiDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=24, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=24, pin_memory=True)\n",
    "\n",
    "# Initialize the model and wrap it with DataParallel if multiple GPUs are available\n",
    "model = LargeNN(input_size).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "# Training loop with TF32 enabled\n",
    "def train_model(model, train_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch [{epoch+1}/{num_epochs}]', unit='batch') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                pbar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        scheduler.step(epoch_loss)\n",
    "\n",
    "# Evaluation function for regression\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    print(f\"Test MSE: {mse:.2f}\")\n",
    "    print(f\"Test RMSE: {mse ** 0.5:.2f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model_LargeNN.pth')\n",
    "\n",
    "print(\"Training and evaluation completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1062312,
     "sourceId": 1793773,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30042,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
